This literature review will examine the theoretical evolution of standard asset pricing models, beginning with the single-factor Capital Asset Pricing Model (CAPM) and extending to the multi-factor Fama-French frameworks. It will highlight the limitations of traditional statistical approaches in asset pricing and explore how advancements in machine learning methodologies may address these shortcomings. Then, the existing literature of the proposed enhancements to the asset pricing models will be reviewed. Specifically, the focus will be on the justification for incorporating liquidity risk and investor sentiment as explanatory factors. The ways in which contemporary researchers integrate these factors into their models will also be discussed. Furthermore, the application of asset pricing models within a sector rotation strategy will be explored, including an assessment of how this approach has been implemented in recent studies.


\subsection{A review of CAPM and its development}

The one factor Capital Asset Pricing Model (CAPM) of \citeA{sharpe_1964} was the first rigorious asset pricing framework, which relates an asset expected return to its market beta. Market beta measures the asset's systematic risk, or its sensitivity to fluctuations in the overall market. An assumption that CAPM makes is that investors hold mean-variance-efficient portfolios, or portfolios that offers the highest expected return for a given level of risk or, conversely, the lowest risk for a given level of expected return. This leads to the prediction that the expected return of an asset is linearly related to its market beta, with higher beta assets commanding higher expected returns as compensation for their increased exposure to market risk. Despite its theoretical appeal and widespread application due to its simplicity, empirical tests have revealed significant deviations from the model's predictions. Studies have found that a one factor could not explain certain stock return patterns. For example, small market capitalization (small-cap) stocks and high book to market ("value") stocks - or stocks that are undervalued- typically perform better than CAPM predicts \cite{capm_2004}.

In response to CAPM's shortcomings, \citeA{ff3_1993} introduced a three-factor model(FF3 hereafter) adding Size (SMB, small minus big) and Value (HML, high minus low book-to-market) factors in addition to the market factor. The SMB factor captures the excess returns of small-cap stocks over large-cap stocks, reflecting how higher volatility and growth potential could cause smaller firms to yield higher average returns. HML accounts for the premium earned by stocks with high book-to-market ratios, often associated with underperforming firms that carry higher risk and, consequently, higher expected returns. The motivation was based on earlier empirical findings that showed firm size and book-to-market equity robustly predict average returns, even when beta does not. By constructing portfolios to mimic these risk factors, the FF3 model significantly improved explanatory power. Indeed, in tests on U.S. stocks, the FF3 alphas (or the intercept of the linear regression model) were near zero, indicating that market, size, and value together “do a good job explaining the cross-section of average stock returns”. Size and value removes systematic mispricing present in the CAPM model. However, the FF3 still left some factors unexplained. \citeA{titman_2004} showed that firms with higher capital investments tend to experience lower future returns, a pattern not captured by the Fama-French three factors. \citeA{novymarx_2013} also found that the profitability of a firm could explain its expected return, similar to book-to-market ratio. Furthermore, some argue that there should be a momentum factor included in the asset pricing model.

\citeA{cahart_1997} introduces an extension of the FF3 by adding a momentum factor (PR1YR), which is the  the prior 12-month return momentum (winners minus losers) as an additional factor. The inclusion of the momentum factor is motivated by empirical evidence, which observes that stocks that have performed well in the past tend to continue outperforming, while past losers continue to underperform. Studies have shown that the size and value factors could not capture this effect. The Carhart 4 factor model (C4F hereafter) became a standard extension when the four factors could describe most of the cross-sectional returns. However, \citeA{huij_2009} indicates that the Carhart model proxies fail to account for real-world constraints. As a result, the premiums associated with the HML and momentum (PR1YR) factors tend to be misestimated.

With new emerging problems, \citeA{ff5_2015} propose a five-factor model (FF5 hereafter), adding profitability (robust minus weak, RMW) and investment (conservative minus aggressive, CMA) factor. RMW accounts for the empirical observation that firms with higher operating profitability tend to earn higher average returns. CMA captures the tendency of firms to aggressively invest in new assets, expecting higher returns in the long run. However, investors may overpay for such firms due to optimism, which results in lower subsequent returns \cite{titman_2004}. The model does not include a momentum factor, as \citeA{ff5_2015} argue that momentum returns are largely short-term and difficult to reconcile with their asset pricing framework, which focuses on long-term risk premia. This update was motivated by research showing that firms with higher profitability or more conservative investment tend to earn higher returns. Interestingly, Fama and French in their own study found that the HML factor became less important in the presence of the new factors.  However, the five-factor model also had its limitations. It did not explicitly include momentum (so momentum remained an “external” anomaly), and it struggled with certain corner cases for instance, it failed to explain the low returns on small stocks that invest a lot despite low profitability. Both \citeA{sarwarff5} and \citeA{benammar_2018} concluded that the FF5 model cannot explain
the average returns of US stocks. \citeA{cakici_2015} argued that the two additional factors are almost non-existent in large firms. With that said, there are tons of evidence of FF5's validity and robust explanatory power, in both national and regional studies \cite{sohor_litreview_2024}.

In sum, over decades the asset pricing model development (CAPM $\rightarrow$ FF3 $\rightarrow$ C4F $\rightarrow$ FF5) was driven by the need to address empirical gaps. Each new factor was added to account for a systematic return pattern unexplained by prior models. While these multifactor models capture much of the cross-section, research continues to find gaps, suggesting even more factors may be needed.


\subsection{Role of machine learning in factor models}

Despite the discourse between which model perfoms the best, one common characteristic they all have is that they are all linear models, which could potentially struggle with complex interactions or non-linear effects among predictors. \citeA{mcdonald_1962} highlights that traditional factor analysis methods assume linear relationships between manifest variables and latent factors, yet real-world financial markets often exhibit nonlinear interactions that linear models fail to capture. The author solidify this claim with another paper in 1983, which argue that factor models with polynomial regression functions, including interaction terms, provide a better framework that can accommodate these relationships \cite{mcdonald_1983}. As the research in ML produce better methods, the wave of its application in asset pricing started to come in. \citeA{hutchinson_1994}  provide one of the earliest applications of machine learning in finance by demonstrating how learning networks can be used for derivative pricing.More recent studies attempted to improve factor models return prediction performance. These ML methods can ingest a wide range of firm characteristics (including standard factors and many others) and capture nonlinear patterns. \citeA{gu_2020} provide a seminal example, applying various ML algorithms to predict the cross-section of U.S. stock returns. They find that flexible models, notably tree-based ensembles and neural networks substantially outperform linear methods in forecasting returns. \citeA{freyberger_2018} similarly show that a non-parametric ML approach uses far fewer predictors than a linear model yet attains a much higher out-of-sample Sharpe ratio indicating less overfitting and better true predictive power.

However, an improved predictive performance within ML models often comes with a decrease in interpretability,  leading to what is commonly known as the "black box" problem.Black-box models refer to algorithms whose internal decision-making processes are opaque or difficult for humans to understand. This is problematic in sectors where regulatory requirements and risk management demand transparency and accountability in decision-making \cite{brozek_2024}.  Moreover, financial markets are dynamic, and models trained on historical data may fail when market conditions change, a phenomenon known as "model drift". Therefore, it is essential to diagnose errors, biases or overfitting to historical data, which these black-box models cannot inherently do \cite{cohen_2021}. \citeA{lipton_2018} even critiques the common perception of linear models being inherently interpretable while deep neural networks are not. The author argues that interpretability depends on context, model complexity, and the availability of meaningful explanations. Several methods have been developed to mitigate the black-box problem by enhancing interpretability without significantly compromising predictive power. Model-specific approaches include decision trees and rule-based models, which allow users to enjoy the predictive power of black-box models while offering human-readable explanations. Feature importance techniques, such as SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations), analyze how individual input features contribute to model predictions. Other methods attempts to extract rules from the black box, such as surrogate trees which trains a more interpretable tree from the predictions of an ensemble.

PAPER BY TRAN TRONG HUYNH

Building upon this body of research, \citeA{simonian_2019} introduced a ML approach to factor modeling by using the Random Forest (RF) algorithm on the C4F model. The authors argue that traditional factor models, including those used in commercial risk platforms, suffer from redundancy created by nonlinearity and multicollinearity. \citeA{simonian_2019} chose the RF model due to how RF could get rid of multicollinearity concerns while allowing for complex interactions. Furthermore, with the RF implementation, the author departs from the classical portfolio-sorting methodology, where stocks are first classified into groups (e.g., small-cap vs. large-cap) before factor loadings are estimated. Instead of forming factor-mimicking portfolios, RF treats each observation individually and evaluates how different factors contribute to returns at various points in time. To address the interpretability challenges inherent in ML-based factor models, \citeA{simonian_2019} incorporated feature importance metrics as an interpretable structure akin to $\beta$ coefficients in traditional factor model regressions. Additionally, they propose a method to construct pseudo-betas by weighting the raw factor elasticities with their respective relative feature importance (RFI hereafter) scores, effectively translating the RF model's output into a form that is more familiar to practitioners in finance. Beyond risk factor analysis, the interpretability of RF-derived predictions is used in a sector rotation strategy. Trading rules are established using RF predicted sector future return alongside volatility ratio signal. If both the predicted return exceeds a threshold and the short-term volatility is lower than long-term volatility, the sector is included in the portfolio. This research demonstrated how black-box models can be structured to provide actionable investment decisions while maintaining interpretability.

\subsection{Emerging additions to factor models}
An attractive feature of ML-enhanced models is their ability to incorporate a wide array of features that traditional econometrics models cannot - at least without constricting assumptions. This allows for the opportunity to include more factors within the factor models. Two factors that are frequently highlighted in the literature as valuable additions to factor models are liquidity risk and investor sentiment. The following discussion examines why these two factors are considered for asset pricing models and the findings when they are incorporated.

\subsubsection{Liquidity Risk}
%%%%%Why liquidity is considered for asset pricing
A liquid investment can be bought and sold quickly without a significant change in its price. Liquidity risk occurs when the same investment faces an imbalance of buyers and sellers in the market or when external factors cause price volatility. \citeA{pastor_2003} provide strong empirical evidence that marketwide liquidity is a state variable that influences expected stock returns. Their study finds that stocks with higher sensitivities to liquidity fluctuations comes with a premium, as investors require additional compensation for holding assets that become difficult to trade when there is a decrease in liquidity. When liquidity "dries up" in the market, liquidity risk amplifies transaction costs and price impacts. Then, investors that use leverage or have liquidity constraints are almost "forced" into liquidating with a premium, further reinforce the pricing of liquidity risk. The authors also conducted a study on portfolios sorted on liquidity betas and found that there is still a significant spread on return, even after controlling for factors within the C4F model. \citeA{amihud_1986} argue that liquidity matters in asset pricing due to the bid-ask spread, which can represent the transaction cost that an investor can incur. Longer-term investors are more willing to hold less liquid assets because they amortize transaction costs over an extended period. Investors with shorter holding periods incur these trading costs more frequently, leading them to demand higher expected returns as compensation for illiquidity. The mechanism is simple: the more friction there is in trading securities, the more investors will want for holding them.\citeA{amihud_1986} confirm that assets with higher bid-ask spreads yield higher return, which adds evidence to liquidity being priced in the market. \citeA{amihud_2002} reinforced the findings, showing that when marketwide illiquidity is anticipated to rise, investors require higher forecasted stock returns. This occurs because higher expected illiquidity implies higher transaction costs and greater uncertainty, leading investors to discount prices accordingly. Additionally, higher status quo illiquidity raises expectations of future illiquidity, thereby increasing required returns and driving prices down. There has also been empirical evidence in international markets that CAPM and FF3 fail to explain stock returns in markets with significant liquidity premium such as Korea \cite{jang_2012}.Given this evidence, liquidity is not only a significant determinant of stock returns but also a systematic risk factor that should be incorporated into asset pricing models to improve their explanatory power 


%%% How are liquidity being implemented in asset pricing
\citeA{acharya_2005} extended the traditional CAPM by incorporating liquidity risk, stating that market risk must be accompanied by three additional risk factors: commonality in liquidity with the market, sensitivity of asset returns to market-wide liquidity fluctuations, and the tendency for an asset's liquidity to decline when the market is in distress. The authors find that the liquidity adjusted CAPM explains asset returns better than the standard CAPM, though not being able to fully explain the book-to-market effect. Findings by \citeA{amihud_1986,amihud_2002,pastor_2003} are also further reinforced. While \citeA{acharya_2005} introduced a liquidity-adjusted CAPM,\citeA{brennan_1998} explore how liquidity directly impacts returns through trading volume as a proxy of liquidity. The authors employ Fama-MacBeth regressions on individual securities rather than portfolios, isolating liquidity effects while adjusting for \citeA{ff3_1993} factors. The results further confirms that liquidity risk is priced in the market. The study finds that including trading volume in the asset pricing model reduces the SMB effect, as smaller firms are typically more illiquid. \citeA{li_2019} further investigates the findings of \citeA{pastor_2003}, successfully replicating their liquidity factor based on historical betas, but found that it is only marginally significant when controlling for other factors. The authors constructed tradable liquidity risk factors based on historical liquidity betas and predicted liquidity risk. When added to FF3, FF5 and CF4, predicted liquidity factor is not significant in any models, while historical liquidity betas is significant for FF3 and C4F. However, \citeA{li_2019} still concluded that asset pricing models are not substantially improved by adding a liquidity risk factor. \citeA{jang_2012} constructed a two-factor model specifically to explain the liquidity premium within the Korean stock market, which cannot be explained with CAPM or FF3. The proposed two factor model include market risk beta (as in CAPM) and a liquidity factor based on liquidity mimicking portfolio,. In this specific market, the CAPM anomaly of small-cap stocks perfoming better than large-cap disappear, implying that the size premium is partially driven by liquidity risk. Furthermore, the liquidity factor captures the variation in returns among high and low book-to-market stocks, meaning that it better explains the value effect. This liquidity risk adjusted CAPM model even captured the anomalies better during the Asian financial crisis than both the CAPM and FF3.

\subsubsection{Investors' sentiment}

Investor sentiment is ....

Baker and Wurgler construct quantitative sentiment index. They aggregated several proxies (such as the volume of IPOs, first-day IPO returns, the equity share in new issues, market turnover, the closed-end fund discount, etc.) into a single sentiment indicator using principal component analysis. This index captures waves of excessive optimism or pessimism in the market. aker and Wurglers key insight was that sentiment affects certain stocks more than others: “when beginning-of-period proxies for sentiment are low, subsequent returns are relatively high” for stocks that are speculative and hard to arbitrage (small, young, high-volatility, unprofitable, non-dividend-paying, or distressed stocks)Conversely, when sentiment is high, those same categories of stocks earn low future returns . during euphoric periods, speculative stocks become overpriced (and later underperform), whereas after pessimistic periods they are underpriced (and subsequently outperform). This pattern suggests that sentiment-driven mispricing occurs and then corrects, especially for stocks that are difficult for arbitrageurs to short or value objectively.Classic finance theory would predict no role for sentiment (prices = fundamental value), but Baker and Wurgler provide evidence that broad waves of optimism/pessimism can create return predictability across stock types, beyond what risk factors explain.





iterative add factor de xem la feature importance co doi ko 